{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bAkKH9jJhRb",
        "outputId": "d2f266eb-25f4-40ff-a123-cec08596e434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEoQ_HI-JtXy",
        "outputId": "a51d4c6a-af54-4472-cde1-bb7d55cc61e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.10/dist-packages (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFYX-wO-J6x5",
        "outputId": "5f20cbb7-4454-4290-f2f1-02e2d5d39815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nvcc4jupyter extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc4jupyter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile activation_functions.h\n",
        "#ifndef ACTIVATION_FUNCTIONS_H\n",
        "#define ACTIVATION_FUNCTIONS_H\n",
        "\n",
        "float sigmoid_cpu(float x);\n",
        "__device__ float sigmoid(float x);\n",
        "__global__ void sigmoid_kernel(float *input, float *output);\n",
        "\n",
        "void sigmoid(float *input, float *output);\n",
        "#endif  // ACTIVATION_FUNCTIONS_H\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOKgbdtC6V4b",
        "outputId": "27431049-9e65-4783-bcf5-83b31dc6425b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting activation_functions.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile feedforward.h\n",
        "#ifndef FEEDFORWARD_H\n",
        "#define FEEDFORWARD_H\n",
        "\n",
        "\n",
        "void feedforward_cpu(float *input, float *output, float *weights, float *biases, int input_size, int output_size);\n",
        "__global__ void feedforward(float *input, float *output, float *weights, float *biases, int input_size, int output_size);\n",
        "\n",
        "#endif  // FEEDFORWARD_H\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1tmQM-B6hrU",
        "outputId": "aecab20d-f55a-4ad4-fea2-b0499caafadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting feedforward.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile initialization.h\n",
        "#ifndef INITIALIZATION_H\n",
        "#define INITIALIZATION_H\n",
        "\n",
        "__global__ void initialize_floats(float *data, int size, float scale);\n",
        "__global__ void initialize_biases(float *biases, int size);\n",
        "\n",
        "#endif  // INITIALIZATION_H\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGWBXbXF6nEo",
        "outputId": "16a4828a-d9a7-4dfb-9654-5072fe9e7f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting initialization.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utilities.h\n",
        "#ifndef UTILITIES_H\n",
        "#define UTILITIES_H\n",
        "\n",
        "#include <cuda_runtime_api.h>\n",
        "\n",
        "void checkCudaError(cudaError_t error, const char* task);\n",
        "\n",
        "#endif  // UTILITIES_H\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmcjPwsc6r52",
        "outputId": "306e2798-fdff-4dc1-a824-c8f8ef031a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utilities.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile activation_functions.cu\n",
        "\n",
        "#include \"activation_functions.h\"\n",
        "#include <math.h>\n",
        "\n",
        "__device__ float sigmoid(float x) {\n",
        "    return 1.0 / (1.0 + exp(-x));\n",
        "}\n",
        "\n",
        "float sigmoid_cpu(float x) {\n",
        "    return 1.0 / (1.0 + exp(-x));\n",
        "}\n",
        "\n",
        "// Define a CUDA kernel\n",
        "__global__ void sigmoid_kernel(float *input, float *output) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < 1000) {\n",
        "        output[idx] = sigmoid(input[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Define a function to launch the CUDA kernel\n",
        "void sigmoid(float *input, float *output) {\n",
        "    int numThreads = 256;\n",
        "    int numBlocks = (1000 + numThreads - 1) / numThreads;\n",
        "    sigmoid_kernel<<<numBlocks, numThreads>>>(input, output);\n",
        "    cudaDeviceSynchronize();\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlLSLkBO6yen",
        "outputId": "d174423a-01c2-40b2-fa79-b206052e79f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting activation_functions.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile feedforward.cu\n",
        "#include \"feedforward.h\"\n",
        "#include \"activation_functions.h\"\n",
        "\n",
        "\n",
        "\n",
        "void feedforward_cpu(float *input, float *output, float *weights, float *biases, int input_size, int output_size) {\n",
        "    // Implementation here\n",
        "    for (int j = 0; j < output_size; j++) {\n",
        "        float sum = 0;\n",
        "        for (int i = 0; i < input_size; i++) {\n",
        "            sum += input[i] * weights[j * input_size + i];\n",
        "        }\n",
        "        output[j] = sigmoid_cpu(sum + biases[j]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void feedforward(float *input, float *output, float *weights, float *biases, int input_size, int output_size) {\n",
        "    // Implementation here\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float sum = 0;\n",
        "        for (int i = 0; i < input_size; i++) {\n",
        "            sum += input[i] * weights[idx * input_size + i];\n",
        "        }\n",
        "        output[idx] = sigmoid(sum + biases[idx]);\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeN2wHxU625Z",
        "outputId": "f5594079-dd58-407f-ac44-73cbbb3b620a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting feedforward.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile initialization.cu\n",
        "#include \"initialization.h\"\n",
        "#include <curand.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "// GPU initialization kernel for weights and input\n",
        "__global__ void initialize_floats(float *data, int size, float scale) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        curandState state;\n",
        "        curand_init(5678, idx, 0, &state);\n",
        "        data[idx] = curand_uniform(&state) * scale;\n",
        "    }\n",
        "}\n",
        "\n",
        "// GPU initialization kernel for biases\n",
        "__global__ void initialize_biases(float *biases, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        biases[idx] = 0.0;\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHjaoKCX7Fn3",
        "outputId": "ab6401c7-5580-448b-b540-40d4df7fe885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting initialization.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utilities.cu\n",
        "#include \"utilities.h\"\n",
        "#include <stdio.h>\n",
        "\n",
        "void checkCudaError(cudaError_t error, const char* task) {\n",
        "    if (error != cudaSuccess) {\n",
        "        printf(\"CUDA error after %s: %s\\n\", task, cudaGetErrorString(error));\n",
        "        exit(1);\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YSqJKte7Vkv",
        "outputId": "4b564ca0-24dc-4cbe-874f-713939be404c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utilities.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <stdio.h>\n",
        "#include <chrono>\n",
        "#include \"activation_functions.h\"\n",
        "#include \"feedforward.h\"\n",
        "#include \"initialization.h\"\n",
        "#include \"utilities.h\"\n",
        "\n",
        "int main() {\n",
        "    int input_neurons = 784;\n",
        "    int hidden_neurons = 2048;\n",
        "    int output_neurons = 10;\n",
        "\n",
        "    // Memory allocation for input, output, weights, and biases\n",
        "    float *d_input, *d_hidden_output, *d_output;\n",
        "    float *d_weights_input_to_hidden, *d_weights_hidden_to_output;\n",
        "    float *d_biases_hidden, *d_biases_output;\n",
        "    float *h_input = new float[input_neurons];\n",
        "    float *h_output = new float[output_neurons];\n",
        "    float *h_hidden_output = new float[hidden_neurons];\n",
        "    float *h_weights_input_to_hidden = new float[input_neurons * hidden_neurons];\n",
        "    float *h_weights_hidden_to_output = new float[hidden_neurons * output_neurons];\n",
        "    float *h_biases_hidden = new float[hidden_neurons];\n",
        "    float *h_biases_output = new float[output_neurons];\n",
        "\n",
        "    checkCudaError(cudaMalloc(&d_input, input_neurons * sizeof(float)), \"allocating d_input\");\n",
        "    checkCudaError(cudaMalloc(&d_hidden_output, hidden_neurons * sizeof(float)), \"allocating d_hidden_output\");\n",
        "    checkCudaError(cudaMalloc(&d_output, output_neurons * sizeof(float)), \"allocating d_output\");\n",
        "    checkCudaError(cudaMalloc(&d_weights_input_to_hidden, input_neurons * hidden_neurons * sizeof(float)), \"allocating d_weights_input_to_hidden\");\n",
        "    checkCudaError(cudaMalloc(&d_weights_hidden_to_output, hidden_neurons * output_neurons * sizeof(float)), \"allocating d_weights_hidden_to_output\");\n",
        "    checkCudaError(cudaMalloc(&d_biases_hidden, hidden_neurons * sizeof(float)), \"allocating d_biases_hidden\");\n",
        "    checkCudaError(cudaMalloc(&d_biases_output, output_neurons * sizeof(float)), \"allocating d_biases_output\");\n",
        "\n",
        "    // Initialize input, weights, and biases on GPU\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (input_neurons + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    initialize_floats<<<blocksPerGrid, threadsPerBlock>>>(d_input, input_neurons, 1.0);\n",
        "    blocksPerGrid = (input_neurons * hidden_neurons + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    initialize_floats<<<blocksPerGrid, threadsPerBlock>>>(d_weights_input_to_hidden, input_neurons * hidden_neurons, 0.1);\n",
        "    blocksPerGrid = (hidden_neurons * output_neurons + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    initialize_floats<<<blocksPerGrid, threadsPerBlock>>>(d_weights_hidden_to_output, hidden_neurons * output_neurons, 0.1);\n",
        "    blocksPerGrid = (hidden_neurons + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    initialize_biases<<<blocksPerGrid, threadsPerBlock>>>(d_biases_hidden, hidden_neurons);\n",
        "    blocksPerGrid = (output_neurons + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    initialize_biases<<<blocksPerGrid, threadsPerBlock>>>(d_biases_output, output_neurons);\n",
        "\n",
        "    // Setup CUDA events for timing\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Copy weights and biases back to CPU for CPU computation\n",
        "    cudaMemcpy(h_input, d_input, input_neurons * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_weights_input_to_hidden, d_weights_input_to_hidden, input_neurons * hidden_neurons * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_weights_hidden_to_output, d_weights_hidden_to_output, hidden_neurons * output_neurons * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_biases_hidden, d_biases_hidden, hidden_neurons * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_biases_output, d_biases_output, output_neurons * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    // CPU timing\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    feedforward_cpu(h_input, h_hidden_output, h_weights_input_to_hidden, h_biases_hidden, input_neurons, hidden_neurons);\n",
        "    feedforward_cpu(h_hidden_output, h_output, h_weights_hidden_to_output, h_biases_output, hidden_neurons, output_neurons);\n",
        "    auto cpu_stop = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::chrono::duration<double, std::milli> cpu_duration = cpu_stop - cpu_start;\n",
        "\n",
        "    printf(\"CPU processing time: %f milliseconds\\n\", cpu_duration.count());\n",
        "\n",
        "    // Launch kernel for the hidden layer\n",
        "    cudaEventRecord(start);\n",
        "    feedforward<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_hidden_output, d_weights_input_to_hidden, d_biases_hidden, input_neurons, hidden_neurons);\n",
        "\n",
        "    // Launch kernel for the output layer\n",
        "    feedforward<<<blocksPerGrid, threadsPerBlock>>>(d_hidden_output, d_output, d_weights_hidden_to_output, d_biases_output, hidden_neurons, output_neurons);\n",
        "    cudaEventRecord(stop);\n",
        "\n",
        "    // Copy result back to host and print GPU output\n",
        "    cudaMemcpy(h_output, d_output, output_neurons * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaEventSynchronize(stop);\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    for (int i = 0; i < output_neurons; i++) {\n",
        "        printf(\"Output[%d]: %f\\n\", i, h_output[i]);\n",
        "    }\n",
        "\n",
        "    printf(\"GPU processing time: %f milliseconds\\n\", milliseconds);\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_hidden_output);\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_weights_input_to_hidden);\n",
        "    cudaFree(d_weights_hidden_to_output);\n",
        "    cudaFree(d_biases_hidden);\n",
        "    cudaFree(d_biases_output);\n",
        "    delete[] h_input;\n",
        "    delete[] h_output;\n",
        "    delete[] h_hidden_output;\n",
        "    delete[] h_weights_input_to_hidden;\n",
        "    delete[] h_weights_hidden_to_output;\n",
        "    delete[] h_biases_hidden;\n",
        "    delete[] h_biases_output;\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVa4Vefq7ZB1",
        "outputId": "ad80746a-ead5-4c7a-f6a4-1ed1b9acc44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu activation_functions.cu feedforward.cu initialization.cu utilities.cu --relocatable-device-code true -lcudadevrt -o main_cuda\n",
        "!./main_cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9AKudcg7sYG",
        "outputId": "56bc6829-be24-4777-c6bb-2a376d869938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU processing time: 7.598326 milliseconds\n",
            "\n",
            "Output[0]: 0.999995\n",
            "Output[1]: 0.999997\n",
            "Output[2]: 0.999996\n",
            "Output[3]: 0.999998\n",
            "Output[4]: 0.999997\n",
            "Output[5]: 0.999996\n",
            "Output[6]: 0.999996\n",
            "Output[7]: 0.999998\n",
            "Output[8]: 0.999996\n",
            "Output[9]: 0.999997\n",
            "GPU processing time: 0.533728 milliseconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With backpropagation"
      ],
      "metadata": {
        "id": "iOzsRd8Nj9F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "from numba import cuda\n",
        "\n",
        "class CUDANeuralNetwork:\n",
        "    def __init__(self, layers, activation='relu'):\n",
        "        self.layers = layers\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.activation = activation\n",
        "\n",
        "        for i in range(1, len(layers)):\n",
        "            self.weights.append(cp.random.randn(layers[i], layers[i-1]).astype(cp.float32) / cp.sqrt(layers[i-1]))\n",
        "            self.biases.append(cp.zeros((layers[i], 1), dtype=cp.float32))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.zs = []\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            z = cp.dot(self.weights[i], self.activations[-1]) + self.biases[i]\n",
        "            self.zs.append(z)\n",
        "            a = self.apply_activation(z)\n",
        "            self.activations.append(a)\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def apply_activation(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return cp.maximum(0, x)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return 1 / (1 + cp.exp(-x))\n",
        "        elif self.activation == 'tanh':\n",
        "            return cp.tanh(x)\n",
        "\n",
        "    def activation_derivative(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return cp.where(x > 0, 1, 0)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            s = self.apply_activation(x)\n",
        "            return s * (1 - s)\n",
        "        elif self.activation == 'tanh':\n",
        "            return 1 - cp.tanh(x)**2\n",
        "\n",
        "    @staticmethod\n",
        "    @cuda.jit\n",
        "    def parallel_forward(weights, biases, inputs, outputs, activation):\n",
        "        i, j = cuda.grid(2)\n",
        "        if i < outputs.shape[0] and j < outputs.shape[1]:\n",
        "            tmp = 0\n",
        "            for k in range(inputs.shape[0]):\n",
        "                tmp += weights[i, k] * inputs[k, j]\n",
        "            tmp += biases[i, 0]\n",
        "\n",
        "            # Apply activation function\n",
        "            if activation == 0:  # ReLU\n",
        "                outputs[i, j] = max(0, tmp)\n",
        "            elif activation == 1:  # Sigmoid\n",
        "                outputs[i, j] = 1 / (1 + cuda.libdevice.exp(-tmp))\n",
        "            elif activation == 2:  # Tanh\n",
        "                outputs[i, j] = cuda.libdevice.tanh(tmp)\n",
        "\n",
        "    def cuda_forward(self, X):\n",
        "        X_gpu = cuda.to_device(X)\n",
        "        self.activations_gpu = [X_gpu]\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            weights_gpu = cuda.to_device(self.weights[i].get())\n",
        "            biases_gpu = cuda.to_device(self.biases[i].get())\n",
        "            output_shape = (self.layers[i+1], X.shape[1])\n",
        "            output_gpu = cuda.device_array(output_shape, dtype=np.float32)\n",
        "\n",
        "            threads_per_block = (16, 16)\n",
        "            blocks_per_grid = (\n",
        "                (output_shape[0] + threads_per_block[0] - 1) // threads_per_block[0],\n",
        "                (output_shape[1] + threads_per_block[1] - 1) // threads_per_block[1]\n",
        "            )\n",
        "\n",
        "            activation_code = 0 if self.activation == 'relu' else 1 if self.activation == 'sigmoid' else 2\n",
        "\n",
        "            self.parallel_forward[blocks_per_grid, threads_per_block](\n",
        "                weights_gpu, biases_gpu, self.activations_gpu[-1], output_gpu, activation_code\n",
        "            )\n",
        "\n",
        "            self.activations_gpu.append(output_gpu)\n",
        "\n",
        "        return self.activations_gpu[-1].copy_to_host()\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate=0.01):\n",
        "        m = X.shape[1]\n",
        "\n",
        "        # Forward pass\n",
        "        self.forward(X)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.activations[-1] - y\n",
        "        nabla_b = [cp.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [cp.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        nabla_b[-1] = cp.sum(delta, axis=1, keepdims=True) / m\n",
        "        nabla_w[-1] = cp.dot(delta, self.activations[-2].T) / m\n",
        "\n",
        "        for l in range(2, len(self.layers)):\n",
        "            delta = cp.dot(self.weights[-l+1].T, delta) * self.activation_derivative(self.zs[-l])\n",
        "            nabla_b[-l] = cp.sum(delta, axis=1, keepdims=True) / m\n",
        "            nabla_w[-l] = cp.dot(delta, self.activations[-l-1].T) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights = [w - learning_rate * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - learning_rate * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def train(self, X, y, epochs=100, learning_rate=0.01, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, X.shape[1], batch_size):\n",
        "                X_batch = X[:, i:i+batch_size]\n",
        "                y_batch = y[:, i:i+batch_size]\n",
        "                self.backpropagation(X_batch, y_batch, learning_rate)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                loss = self.compute_loss(X, y)\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    def compute_loss(self, X, y):\n",
        "        output = self.forward(X)\n",
        "        return cp.mean(cp.sum((output - y)**2, axis=0))\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the neural network architecture\n",
        "    layers = [784, 128, 64, 10]  # Example for MNIST dataset\n",
        "\n",
        "    # Create the neural network\n",
        "    nn = CUDANeuralNetwork(layers, activation='sigmoid')\n",
        "\n",
        "    # Generate random input data and labels\n",
        "    input_data = np.random.randn(784, 1000).astype(np.float32)  # 1000 samples\n",
        "    labels = np.random.randint(0, 10, size=(10, 1000)).astype(np.float32)\n",
        "    labels = (labels == np.arange(10)[:, np.newaxis]).astype(np.float32)\n",
        "\n",
        "    # Train the network\n",
        "    nn.train(cp.asarray(input_data), cp.asarray(labels), epochs=100, learning_rate=0.1)\n",
        "\n",
        "    # Perform forward pass\n",
        "    output = nn.cuda_forward(input_data)\n",
        "    print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFXsceQDkAYd",
        "outputId": "f45e8b06-ce09-45fa-c655-7aa3b700eedb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.8870215449612883\n",
            "Epoch 10, Loss: 0.8657606718556794\n",
            "Epoch 20, Loss: 0.7732219297377405\n",
            "Epoch 30, Loss: 0.47857357297836933\n",
            "Epoch 40, Loss: 0.19293552188522464\n",
            "Epoch 50, Loss: 0.06031313058716237\n",
            "Epoch 60, Loss: 0.0215947776557352\n",
            "Epoch 70, Loss: 0.009863558819462692\n",
            "Epoch 80, Loss: 0.005371389576487265\n",
            "Epoch 90, Loss: 0.003290592687824068\n",
            "Output shape: (10, 1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 63 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}